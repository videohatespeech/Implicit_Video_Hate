{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8dDgLD-7Hal"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import moviepy\n",
    "import librosa\n",
    "import glob\n",
    "import moviepy.editor as mp\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import matplotlib\n",
    "import tqdm\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "import os\n",
    "from os import walk, listdir\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, BertForSequenceClassification,BertPreTrainedModel, BertModel, BertTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from scipy.io.wavfile import read\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygJvLDiZGOtT"
   },
   "source": [
    "EXTRACTING TEXT FROM AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "libhXX99s64z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "#from datasets import load_dataset\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DylgfYtls35M"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model and processor\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLPPe4s0GORf"
   },
   "outputs": [],
   "source": [
    "# Function to process audio file and return transcription\n",
    "def process_audio_file(audio_file_path):\n",
    "    speech, rate = librosa.load(audio_file_path, sr=16000)  # Preprocess audio using the processor\n",
    "    inputs = processor(speech, sampling_rate=rate, return_tensors=\"pt\")\n",
    "    # Generate transcription\n",
    "    generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    # Decode generated transcription\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLVYy4bMsx-S"
   },
   "outputs": [],
   "source": [
    "# Folder containing audio files\n",
    "folder_path = \"/path/to/folder/containing/videos/\"\n",
    "df = pd.read_excel(f\"{FOLDER_DIR}/{CATEGORY}.xlsx')\n",
    "\n",
    "# List to store transcriptions\n",
    "transcriptions = []\n",
    "file_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UaSw-4W7szgN",
    "outputId": "32ae2dc8-253e-4461-d446-5e3ee8f87db9"
   },
   "outputs": [],
   "source": [
    "# Iterate over each audio file in the folder\n",
    "i=1\n",
    "\n",
    "for file_name in df['Video_ID']:\n",
    "    file_name = file_name + '.wav'\n",
    "    try :\n",
    "        if file_name.endswith(\".wav\"):  # Assuming all audio files are in .wav format\n",
    "            audio_file_path = os.path.join(folder_path, file_name)\n",
    "            transcription = process_audio_file(audio_file_path)\n",
    "            transcriptions.append(transcription)\n",
    "            file_names.append(file_name)\n",
    "            print(i, file_name)\n",
    "            i=i+1\n",
    "    except:\n",
    "            i =i+1\n",
    "            print('error in',i, file_name)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TyRbaMZs3Jp"
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"file_name\":file_names, \"Transcription\": transcriptions})\n",
    "\n",
    "# Save DataFrame as CSV\n",
    "df.to_csv(f\"transcriptions_{CATEGORY}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
