{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir='/usr/lib/cuda'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4MfuuEoeSuM",
    "outputId": "d69d741a-68d2-4ac6-b1df-79073440ffb2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CUDA_HOME $LD_LIBRARY_PATH $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_HOME=/usr/local/cuda\n",
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_HOME/lib64\n",
    "!export PATH=$PATH:$CUDA_HOME/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0cE262ejUM1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import keras\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMAGE BIND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd  /home/ankr/Notebook/HateMM/ImageBind/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagebind import data\n",
    "import torch\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/home/ankr/Notebook/HateMM/Own_Explicit_Hate_Videos.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eht_df = pd.read_csv('/home/ankr/Notebook/transcriptions_OWN_EXPLICIT_HATE_VIDEOS.csv')\n",
    "\n",
    "eht_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExporter(torch.nn.Module):\n",
    "    def __init__(self, model, modality):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.modality = modality\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model({self.modality: data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FOLDER_NAME = '/home/ankr/Notebook/'\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Iterate over filenames in df and retrieve transcriptions\n",
    "for filename in df['Video_ID'][137:138]:\n",
    "    print(f\"Processing {filename}\")\n",
    "    text_list = []\n",
    "    file = filename + '.wav'\n",
    "    transcription = eht_df.loc[eht_df['file_name'] == file, 'Transcription'].values\n",
    "    if len(transcription) > 0:\n",
    "        for j in range(explicit_hate_frames[count]):\n",
    "            text_list.append(transcription)\n",
    "    else:\n",
    "        print(f\"No transcription found for {file}\")\n",
    "\n",
    "    print(\"Text completed\")\n",
    "    \n",
    "    #audio_paths must be paths to audio files\n",
    "    audio_paths = []\n",
    "    file = filename + '.wav'\n",
    "    for j in range(explicit_hate_frames[count]):\n",
    "        audio_paths.append('/home/ankr/Notebook/HateMM/Own Videos Audio Files/Explicit Hate/' + file)\n",
    "\n",
    "    print(\"Audio completed\")\n",
    "    \n",
    "    #image_paths should be paths to images\n",
    "    image_paths = []\n",
    "    for i in range(len(os.listdir('/home/ankr/Notebook/HateMM/Own Image Frames/Explicit Hate Frames/' + filename))):\n",
    "        image_paths.append('/home/ankr/Notebook/HateMM/Own Image Frames/Explicit Hate Frames/' + filename + '/' + f'frame_{i}'+ '.jpg')\n",
    "\n",
    "    print(\"Image completed\")\n",
    "\n",
    "    # Convert all elements in text_list to strings\n",
    "    text_list = [str(text) for text in text_list]\n",
    "\n",
    "    inputs = {\n",
    "        ModalityType.TEXT: data.load_and_transform_text(text_list, 'cpu'),\n",
    "        ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, 'cpu'),\n",
    "        ModalityType.VISION: data.load_and_transform_vision_data(image_paths, 'cpu'),\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "        \n",
    "    print(\"Data loading done\")\n",
    "    ov_modality_models = {}\n",
    "\n",
    "    modalities = [ModalityType.TEXT, ModalityType.VISION, ModalityType.AUDIO]\n",
    "    for modality in modalities:\n",
    "        export_dir = Path(f\"image-bind-{modality}\")\n",
    "        file_name = f\"image-bind-{modality}\"\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        ir_path = export_dir / f\"{file_name}.xml\"\n",
    "        if not ir_path.exists():\n",
    "            exportable_model = ModelExporter(model, modality)\n",
    "            model_input = inputs[modality]\n",
    "            ov_model = ov.convert_model(exportable_model, example_input=model_input)\n",
    "            ov.save_model(ov_model, ir_path)\n",
    "        else:\n",
    "            ov_model = core.read_model(ir_path)\n",
    "        ov_modality_models[modality] = core.compile_model(ov_model, device.value)\n",
    "        \n",
    "    embeddings = {}\n",
    "    for modality in modalities:\n",
    "        embeddings[modality] = ov_modality_models[modality](inputs[modality])[ov_modality_models[modality].output(0)]\n",
    "        \n",
    "    count = count+1\n",
    "    print(\"Completed: \" , count)\n",
    "    file_to_store = f\"/home/ankr/Notebook/HateMM/Own Videos ImageBind Embeddings/Explicit Hate/OWN_imagebind_EXPLICIT_HATE_embeddings_{count}.npy\"\n",
    "    \n",
    "    # Save the embedding to file\n",
    "    np.save(file_to_store, embeddings)\n",
    "        \n",
    "    # Getting shape and size of each array\n",
    "    text_shape = embeddings['text'].shape\n",
    "    text_size = embeddings['text'].size\n",
    "\n",
    "    vision_shape = embeddings['vision'].shape\n",
    "    vision_size = embeddings['vision'].size\n",
    "\n",
    "    audio_shape = embeddings['audio'].shape\n",
    "    audio_size = embeddings['audio'].size\n",
    "\n",
    "    print(\"Text Shape:\", text_shape)\n",
    "    print(\"Text Size:\", text_size)\n",
    " \n",
    "    print(\"Vision Shape:\", vision_shape)\n",
    "    print(\"Vision Size:\", vision_size)\n",
    "\n",
    "    print(\"Audio Shape:\", audio_shape)\n",
    "    print(\"Audio Size:\", audio_size)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
