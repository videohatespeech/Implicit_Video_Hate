{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir='/usr/lib/cuda'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aezZU_ZY2Kzz"
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import OFATokenizer, OFAModel\n",
    "from transformers.models.ofa.generate import sequence_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ_PJYz12WJT"
   },
   "outputs": [],
   "source": [
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "resolution = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUZWnl9u2cQu"
   },
   "outputs": [],
   "source": [
    "patch_resize_transform = transforms.Compose([\n",
    "    lambda image: image.convert(\"RGB\"),\n",
    "    transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NtAuUw12i_M",
    "outputId": "dac9f1a5-8156-4da9-810d-4b2a344dc64c"
   },
   "outputs": [],
   "source": [
    "ckpt_dir='OFA-tiny'\n",
    "tokenizer = OFATokenizer.from_pretrained(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uau2BJUj29cb",
    "outputId": "0cde9e83-8117-48be-d6e5-ab3cbf6f4558"
   },
   "outputs": [],
   "source": [
    "model = OFAModel.from_pretrained(ckpt_dir, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65DcnoJB3Qt_"
   },
   "outputs": [],
   "source": [
    "generator = sequence_generator.SequenceGenerator(\n",
    "    tokenizer=tokenizer,\n",
    "    beam_size=5,\n",
    "    max_len_b=16,\n",
    "    min_len=0,\n",
    "    no_repeat_ngram_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = '/path/to/folder/containing/videos'\n",
    "IMAGE_DIR = '/path/to/folder/containing/image frames'\n",
    "CATEGORY =  \"Explicit Hate Videos\" #change it to Implicit Hate Videos, Non Hate Videos as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folders = []\n",
    "captions = []\n",
    "\n",
    "df1 = pd.read_excel(f'{FOLDER_NAME}/{CATEGORY}.xlsx') #Excel file with video names and IDs\n",
    "\n",
    "txt = \" what does the image describe?\"\n",
    "inputs = tokenizer([txt], return_tensors=\"pt\").input_ids\n",
    "\n",
    "for filename in df1['Video_ID']:\n",
    "    if(filename in os.listdir(f'{IMAGE_DIR}/{CATEGORY}')):\n",
    "        file_path = f'{IMAGE_DIR}/{CATEGORY}' + filename\n",
    "        list_of_frames = os.listdir(file_path)\n",
    "        for i in range(0, len(list_of_frames), 4):\n",
    "            path = file_path + '/' + list_of_frames[i]\n",
    "            img = Image.open(path)\n",
    "            patch_img = patch_resize_transform(img).unsqueeze(0)\n",
    "            gen = model.generate(inputs, patch_images=patch_img, num_beams=5, no_repeat_ngram_size=3)\n",
    "            captions.append(tokenizer.batch_decode(gen, skip_special_tokens=True)[0].strip())\n",
    "            image_folders.append(filename)\n",
    "        print(f'{filename} captions complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = '/path/to/folder/for/storing/captions'\n",
    "files = []\n",
    "for filename in (os.listdir(destination_folder)):\n",
    "    filename = filename.replace('.npy', '')\n",
    "    files.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df = pd.DataFrame({\n",
    "    'Video': files,\n",
    "    'Caption': captions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df.to_csv(f'{destination_folder}/{CATEGORY} captions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate BERT embeddings for a list of texts\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = tokenizer(text_list, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.pooler_output.detach().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = pd.read_csv(f'{destination_folder}/{CATEGORY} captions.csv')\n",
    "captions = captions['Caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i in range(len(captions)):\n",
    "    embeddings.append(get_bert_embeddings(captions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2= arr1.reshape(35919,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_embeddings(embeddings):\n",
    "    # Apply max pooling along the token dimension (axis 1)\n",
    "    pooled_embeddings, _ = torch.max(embeddings, dim=1)\n",
    "    return pooled_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply max pooling to a single embedding tensor\n",
    "def max_pool_single_embedding(embedding):\n",
    "    # Convert NumPy array to PyTorch tensor if necessary\n",
    "    if isinstance(embedding, np.ndarray):\n",
    "        embedding = torch.tensor(embedding)\n",
    "    # Apply max pooling along the token dimension (axis 0)\n",
    "    pooled_embedding, _ = torch.max(embedding, dim=0)\n",
    "    return pooled_embedding\n",
    "\n",
    "# Function to apply max pooling to a list of embeddings\n",
    "def max_pool_embeddings_list(embeddings):\n",
    "    # Apply max pooling to each tensor in the list\n",
    "    pooled_embeddings = [max_pool_single_embedding(embedding) for embedding in embeddings]\n",
    "    return torch.stack(pooled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_stack = []\n",
    "for i in range(len(embeddings)):\n",
    "    emb_stack.append(max_pool_single_embedding(embeddings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in missing_index:\n",
    "    arr2[i] = np.full(509, -1000)\n",
    "    image_folders[i] = 'delete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tensors that are entirely filled with -1000\n",
    "filtered_list = [arr for arr in arr2 if not np.all(arr == -1000)]\n",
    "filtered_image_folders = [folder for folder in image_folders if not(folder=='delete')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming image_folders is a list of folder names or IDs corresponding to embeddings\n",
    "max_pool_emb = []\n",
    "\n",
    "i=1\n",
    "temp = []\n",
    "\n",
    "df = pd.read_csv(f'/{destination_folder}/{CATEGORY} captions.csv')\n",
    "\n",
    "cnt = 0\n",
    "while i < len(emb_stack):  # Start loop from index 1 to avoid out-of-bound errors\n",
    "    # Collect embeddings while the folder is the same\n",
    "    if df['Video'][i] == df['Video'][i-1]:\n",
    "        temp.append(arr2[i-1])\n",
    "        i += 1\n",
    "    # Ensure that temp is not empty before stacking\n",
    "    else:\n",
    "        cnt = cnt+1\n",
    "        if temp:\n",
    "            max_pool_emb.append(np.max(temp,axis=0))\n",
    "            temp = []\n",
    "        else:\n",
    "            max_pool_emb.append(arr2[i])\n",
    "        i =i+1\n",
    "\n",
    "max_pool_emb.append(arr2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr4 = np.array(max_pool_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/ankr/Notebook/HateMM/Own Dataset/Transcription Features/OWN_NON_HATE_captions_pooled_embeddings.npy\",arr4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
