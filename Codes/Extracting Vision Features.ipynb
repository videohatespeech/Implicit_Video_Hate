{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8dDgLD-7Hal"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import moviepy\n",
    "import librosa\n",
    "import glob\n",
    "import moviepy.editor as mp\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import matplotlib\n",
    "import tqdm\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "import os\n",
    "from os import walk, listdir\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, BertForSequenceClassification,BertPreTrainedModel, BertModel, BertTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from scipy.io.wavfile import read\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACTING FEATURES FROM IMAGE FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = '/path/to/folder/containing/image frames'\n",
    "CATEGORY = 'Explicit Hate Videos' #change to Implicit Hate Videos, Non Hate Videos as needed\n",
    "FEATURE_DIR = '/path/to/folder/for/storing/features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Initialize model\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the pooler outputs\n",
    "last_hidden_states = []\n",
    "folders =[]\n",
    "minFrames = 100\n",
    "\n",
    "df = pd.read_excel(f'{FOLDER_NAME}/{CATEGORY}.xlsx')\n",
    "cnt = 1\n",
    "\n",
    "# Loop through each folder\n",
    "for folder in (df['Video_ID']):\n",
    "    frames = []  # Store frame embeddings for the current folder\n",
    "    folder_frames = os.listdir(FOLDER_NAME + folder)\n",
    "    num_frames = len(folder_frames)\n",
    "    \n",
    "    if num_frames <= 100:\n",
    "        selected_frames = folder_frames\n",
    "    else:\n",
    "        # Uniformly sample 100 frames\n",
    "        step = int(num_frames/minFrames)\n",
    "        indices = list(range(0, num_frames, step))\n",
    "        selected_frames = [folder_frames[i] for i in indices]\n",
    "        selected_frames = selected_frames[:100]\n",
    "        \n",
    "    for image in selected_frames:\n",
    "        img = Image.open(FOLDER_NAME + folder+'/'+image)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), \n",
    "            transforms.ToTensor(),  \n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        input_tensor = transform(img).unsqueeze(0)  \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=input_tensor)\n",
    "        last_hidden_state = outputs.last_hidden_state.detach().numpy()\n",
    "        frames.append(last_hidden_state)\n",
    "    # If the folder has less than 100 frames, append zeroes\n",
    "    while len(frames) < 100:\n",
    "        frames.append(np.zeros_like(frames[0], dtype=np.float32))\n",
    "    # Stack the frames along the first dimension to get shape (100, 768)\n",
    "    last_hidden_states.append(frames)\n",
    "    print(len(last_hidden_states), len(frames))\n",
    "\n",
    "arr_img = np.array(last_hidden_states)\n",
    "np.save(f\"{FEATURE_DIR}/{CATEGORY}_img_2Dfeatures.npy\",arr_img)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
