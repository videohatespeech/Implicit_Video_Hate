{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37eec5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65ea3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cedd52a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a88c9574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "beec64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9863a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab21143",
   "metadata": {},
   "source": [
    "IMAGEBIND EMBEDDINGS WITH EMOTION AND SENTIMENT SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56e6dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cef4b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shapes for each modality\n",
    "image_input_shape = (1024,)\n",
    "text_input_shape = (1024,)\n",
    "audio_input_shape = (1024,)\n",
    "emotion_input_shape = (10,)\n",
    "sentiment_input_shape = (3,)\n",
    "caption_input_shape = (1024,)\n",
    "cross_input_shape = (256,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5940480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_encoder():\n",
    "    image_input = tf.keras.layers.Input(shape=image_input_shape, name=\"image_input\")\n",
    "    \n",
    "    l = tf.keras.layers.Dense(1024, activation='relu')(image_input)\n",
    "    l = tf.keras.layers.Dropout(0.2)(l)\n",
    "    l = tf.keras.layers.Dense(512, activation='relu')(l)\n",
    "    l = tf.keras.layers.Dropout(0.2)(l)\n",
    "    l = tf.keras.layers.Dense(256, activation='relu')(l)\n",
    "    l = tf.keras.layers.Dropout(0.2)(l)\n",
    "    outputs = l\n",
    "    model = keras.Model(inputs=image_input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "888906a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Encoder\n",
    "def create_text_encoder():\n",
    "    text_input = tf.keras.layers.Input(shape=text_input_shape, name=\"text_input\")\n",
    "\n",
    "    m = tf.keras.layers.Dense(1024, activation='relu')(text_input)\n",
    "    m = tf.keras.layers.Dropout(0.2)(m)\n",
    "    m = tf.keras.layers.Dense(512, activation='relu')(m)\n",
    "    m = tf.keras.layers.Dropout(0.2)(m)\n",
    "    m = tf.keras.layers.Dense(256, activation='relu')(m)\n",
    "    m = tf.keras.layers.Dropout(0.2)(m)\n",
    "    outputs = m\n",
    "    \n",
    "    model = keras.Model(inputs=text_input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32ac46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Encoder\n",
    "def create_audio_encoder():\n",
    "    audio_input = tf.keras.layers.Input(shape=audio_input_shape, name=\"audio_input\")\n",
    "    n = tf.keras.layers.Dense(1024, activation='relu')(audio_input)\n",
    "    n = tf.keras.layers.Dropout(0.2)(n)\n",
    "    n = tf.keras.layers.Dense(512, activation='relu')(n)\n",
    "    n = tf.keras.layers.Dropout(0.2)(n)\n",
    "    n = tf.keras.layers.Dense(256, activation='relu')(n)\n",
    "    n = tf.keras.layers.Dropout(0.2)(n)\n",
    "    outputs = n\n",
    "    model = keras.Model(inputs=audio_input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e320000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Encoder\n",
    "def create_cross_encoder():\n",
    "    cross_input = tf.keras.layers.Input(shape=cross_input_shape, name=\"cross_input\")\n",
    "    n = tf.keras.layers.Dense(256, activation='relu')(cross_input)\n",
    "    n = tf.keras.layers.Dropout(0.2)(n)\n",
    "    n = tf.keras.layers.Dense(128, activation='relu')(n)\n",
    "    n = tf.keras.layers.Dropout(0.2)(n)\n",
    "    outputs = n\n",
    "    model = keras.Model(inputs=cross_input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5439efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion and Sentiment Encoder\n",
    "def create_emotion_sentiment_encoder():\n",
    "    emotion_inputs = tf.keras.layers.Input(emotion_input_shape, name=\"emotion_input\")\n",
    "    sentiment_inputs = tf.keras.layers.Input(sentiment_input_shape, name=\"sentiment_input\")\n",
    "    \n",
    "    #Concatenate the features\n",
    "    inputs = tf.keras.layers.concatenate([emotion_inputs, sentiment_inputs],axis=1)\n",
    "\n",
    "    m = tf.keras.layers.Dense(32, activation='relu', name=\"text_output1\")(inputs)\n",
    "    m = tf.keras.layers.Dropout(0.2, name=\"text_dropout1\")(m)\n",
    "    m = tf.keras.layers.Dense(16, activation='relu', name=\"text_output2\")(m)\n",
    "    m = tf.keras.layers.Dropout(0.2, name=\"text_dropout2\")(m)\n",
    "    outputs = m\n",
    "    \n",
    "    model = keras.Model(inputs=[emotion_inputs, sentiment_inputs], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "edb33007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption Encoder\n",
    "def create_caption_encoder():\n",
    "    caption_input = tf.keras.layers.Input(shape=caption_input_shape, name=\"caption_input\")\n",
    "    n = tf.keras.layers.Dense(1024, activation='relu')(caption_input)\n",
    "    n = tf.keras.layers.Dropout(0.2)(n)\n",
    "    n = tf.keras.layers.Dense(512, activation='relu')(n)\n",
    "    n = tf.keras.layers.Dropout(0.2)(n)\n",
    "    n = tf.keras.layers.Dense(256, activation='relu')(n)\n",
    "    n = tf.keras.layers.Dropout(0.2)(n)\n",
    "    outputs = n\n",
    "    model = keras.Model(inputs=caption_input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b62f2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [16, 32, 64]\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "projection_units_merge = 1024\n",
    "num_of_epochs = [20,30, 50]\n",
    "dropout_rate = 0.5\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b70efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretraining the encoder\n",
    "encoderII = create_image_encoder()\n",
    "encoderTT = create_text_encoder()\n",
    "encoderAA = create_audio_encoder()\n",
    "encoderIT = create_cross_encoder()\n",
    "encoderIA = create_cross_encoder()\n",
    "encoderTI = create_cross_encoder()\n",
    "encoderTA = create_cross_encoder()\n",
    "encoderAI = create_cross_encoder()\n",
    "encoderAT = create_cross_encoder()\n",
    "encoderCP = create_caption_encoder()\n",
    "encoderES = create_emotion_sentiment_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "086f47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(trainable=True):\n",
    "    \n",
    "    for layer in encoderES.layers:\n",
    "        layer.trainable = trainable\n",
    "        \n",
    "    for layer in encoderCP.layers:\n",
    "        layer.trainable = trainable\n",
    "        \n",
    "    # Inputs for each modality\n",
    "    image_input = tf.keras.layers.Input(shape=image_input_shape, name=\"image_input\")\n",
    "    text_input = tf.keras.layers.Input(shape=text_input_shape, name=\"text_input\")\n",
    "    audio_input = tf.keras.layers.Input(shape=audio_input_shape, name=\"audio_input\")\n",
    "    emotion_input = tf.keras.layers.Input(shape=emotion_input_shape, name=\"emotion_input\")\n",
    "    sentiment_input = tf.keras.layers.Input(shape=sentiment_input_shape, name=\"sentiment_input\")\n",
    "    caption_input = tf.keras.layers.Input(shape=caption_input_shape, name=\"caption_input\")\n",
    "        \n",
    "    #Merge emotion and sentiment inputs\n",
    "    merge_emotion_sentiment_input = tf.keras.layers.concatenate([emotion_input, sentiment_input], axis=1)\n",
    "    \n",
    "    # Extract features using the encoder\n",
    "    image_features_II = encoderII(image_input)\n",
    "    audio_features_AA = encoderAA(audio_input)\n",
    "    text_features_TT = encoderTT(text_input)\n",
    "    \n",
    "\n",
    "    #Cross encoders\n",
    "    image_features_IT = encoderIT(image_features_II)\n",
    "    image_features_IA = encoderIA(image_features_II)\n",
    "    text_features_TI = encoderTI(text_features_TT)\n",
    "    text_features_TA = encoderTA(text_features_TT)\n",
    "    audio_features_AI = encoderAI(audio_features_AA)\n",
    "    audio_features_AT = encoderAT(audio_features_AA)\n",
    "    emotion_features_ES = encoderES([emotion_input, sentiment_input])\n",
    "    caption_features_CP = encoderCP(caption_input)\n",
    "     \n",
    "    #Concatenate the features\n",
    "    merge = tf.keras.layers.concatenate([image_features_IT, image_features_IA, text_features_TI, text_features_TA, audio_features_AI,\n",
    "                                        audio_features_AT, emotion_features_ES, caption_features_CP],axis=1)\n",
    "\n",
    "    \n",
    "    # Add classification layers on top of the encoder\n",
    "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(merge)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    # Define the classifier model\n",
    "    model = keras.Model(inputs=[image_input, text_input, audio_input, emotion_input, sentiment_input, caption_input], outputs=outputs, name=\"multimodal_classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "993108c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss_cross(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        \n",
    "        #split\n",
    "        zi, zj = tf.split(feature_vectors, num_or_size_splits=2, axis = 0)\n",
    "        \n",
    "        # Normalize feature vectors\n",
    "        zi_normalized = tf.math.l2_normalize(zi, axis=1)\n",
    "        zj_normalized = tf.math.l2_normalize(zj, axis=1)\n",
    "       \n",
    "        return SupervisedContrastiveLoss_cross.supervised_NT_Xent_tf(zi_normalized, zj_normalized, tf.squeeze(labels))\n",
    "    \n",
    "\n",
    "    # Define the supervised contrastive loss function\n",
    "    def supervised_NT_Xent_tf(zi, zj, labels):\n",
    "        \"\"\"\n",
    "        Calculates the supervised contrastive loss of the input data using NT_Xent.\n",
    "\n",
    "        Args:\n",
    "            zi: One half of the input data, shape = (batch_size, feature_1, feature_2, ..., feature_N)\n",
    "            zj: Other half of the input data, must have the same shape as zi\n",
    "            labels: Tensor of shape (batch_size,) with integer labels representing the class of each pair\n",
    "            tau: Temperature parameter (a constant), default = 1.\n",
    "\n",
    "        Returns:\n",
    "            loss: The complete supervised NT_Xent contrastive loss\n",
    "        \"\"\"\n",
    "        epsilon = 1e-8\n",
    "        batch_size = tf.shape(zi)[0]\n",
    "\n",
    "        # Compute cosine similarities between all pairs in zi and zj\n",
    "        cosine_sim = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
    "        sim_matrix = -cosine_sim(tf.expand_dims(zi, 1), tf.expand_dims(zj, 0))  # Shape: (batch_size, batch_size)\n",
    "\n",
    "        # Optional normalization of the similarity matrix\n",
    "        sim_matrix = (sim_matrix - tf.reduce_min(sim_matrix)) / (tf.reduce_max(sim_matrix) - tf.reduce_min(sim_matrix) + epsilon)\n",
    "\n",
    "        # Create positive and negative masks\n",
    "        positive_mask = tf.equal(tf.expand_dims(labels, 1), tf.expand_dims(labels, 0))\n",
    "        negative_mask = tf.logical_not(positive_mask)\n",
    "\n",
    "        # Calculate loss for each sample pair\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            # Select positive and negative samples\n",
    "            positive_sim = tf.boolean_mask(sim_matrix[i], positive_mask[i])\n",
    "            negative_sim = tf.boolean_mask(sim_matrix[i], negative_mask[i])\n",
    "\n",
    "            # Calculate numerator and denominator with stability enhancements\n",
    "            numerator = tf.reduce_sum(tf.math.exp(positive_sim / temperature))\n",
    "            denominator = tf.reduce_sum(tf.math.exp(negative_sim / temperature)) + epsilon  # Avoid division by zero\n",
    "\n",
    "            # Safeguard against log(0) or log(very small value)\n",
    "            fraction = numerator / (denominator + epsilon)\n",
    "            loss_ij = -tf.math.log(tf.maximum(fraction, epsilon))  # Ensure log argument is never too small\n",
    "            loss += loss_ij\n",
    "\n",
    "        # Average loss over all samples\n",
    "        loss /= tf.cast(batch_size, tf.float32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fefc81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss_self(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        \n",
    "        # Normalize feature vectors\n",
    "        zi_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "       \n",
    "        return SupervisedContrastiveLoss_self.supervised_NT_Xent_tf(zi_normalized, zi_normalized, tf.squeeze(labels))\n",
    "    \n",
    "\n",
    "    # Define the supervised contrastive loss function\n",
    "    def supervised_NT_Xent_tf(zi, zj, labels):\n",
    "        \"\"\"\n",
    "        Calculates the supervised contrastive loss of the input data using NT_Xent.\n",
    "\n",
    "        Args:\n",
    "            zi: One half of the input data, shape = (batch_size, feature_1, feature_2, ..., feature_N)\n",
    "            zj: Other half of the input data, must have the same shape as zi\n",
    "            labels: Tensor of shape (batch_size,) with integer labels representing the class of each pair\n",
    "            tau: Temperature parameter (a constant), default = 1.\n",
    "\n",
    "        Returns:\n",
    "            loss: The complete supervised NT_Xent contrastive loss\n",
    "        \"\"\"\n",
    "        epsilon = 1e-8\n",
    "        batch_size = tf.shape(zi)[0]\n",
    "\n",
    "        # Compute cosine similarities between all pairs in zi and zj\n",
    "        cosine_sim = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
    "        sim_matrix = -cosine_sim(tf.expand_dims(zi, 1), tf.expand_dims(zj, 0))  # Shape: (batch_size, batch_size)\n",
    "\n",
    "        # Optional normalization of the similarity matrix\n",
    "        sim_matrix = (sim_matrix - tf.reduce_min(sim_matrix)) / (tf.reduce_max(sim_matrix) - tf.reduce_min(sim_matrix) + epsilon)\n",
    "\n",
    "        # Create positive and negative masks\n",
    "        positive_mask = tf.equal(tf.expand_dims(labels, 1), tf.expand_dims(labels, 0))\n",
    "        negative_mask = tf.logical_not(positive_mask)\n",
    "\n",
    "        # Calculate loss for each sample pair\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            # Select positive and negative samples\n",
    "            positive_sim = tf.boolean_mask(sim_matrix[i], positive_mask[i])\n",
    "            negative_sim = tf.boolean_mask(sim_matrix[i], negative_mask[i])\n",
    "\n",
    "            # Calculate numerator and denominator with stability enhancements\n",
    "            numerator = tf.reduce_sum(tf.math.exp(positive_sim / temperature))\n",
    "            denominator = tf.reduce_sum(tf.math.exp(negative_sim / temperature)) + epsilon  # Avoid division by zero\n",
    "\n",
    "            # Safeguard against log(0) or log(very small value)\n",
    "            fraction = numerator / (denominator + epsilon)\n",
    "            loss_ij = -tf.math.log(tf.maximum(fraction, epsilon))  # Ensure log argument is never too small\n",
    "            loss += loss_ij\n",
    "\n",
    "        # Average loss over all samples\n",
    "        loss /= tf.cast(batch_size, tf.float32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "289d4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_projection_head_cross(encoder1,encoder2,encoder3, encoder4):\n",
    "    # Inputs for each modality\n",
    "    inputs_1 = tf.keras.layers.Input(shape=(1024,), name=\"input_1\")\n",
    "    inputs_2 = tf.keras.layers.Input(shape=(1024,), name=\"input_2\")\n",
    "        \n",
    "    # Extract features using the encoder\n",
    "    features_1 = encoder1(inputs_1)\n",
    "    features_2 = encoder2(inputs_2)\n",
    "    \n",
    "    features_3 = encoder3(features_1)\n",
    "    features_4 = encoder4(features_2)\n",
    "    \n",
    "    outputs_1 = tf.keras.layers.Dense(projection_units, activation=\"relu\")(features_3)\n",
    "    outputs_2 = tf.keras.layers.Dense(projection_units, activation=\"relu\")(features_4)\n",
    "    output_concat = tf.concat([outputs_1, outputs_2], axis = 0)\n",
    "    model = tf.keras.models.Model(inputs=[inputs_1,inputs_2], outputs=output_concat)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe600226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_projection_head_merge(encoder1, encoder2, encoder3):\n",
    "    # Inputs for each modality\n",
    "    inputs_1 = tf.keras.layers.Input(shape=(1024,), name=\"input_1\")\n",
    "    inputs_2 = tf.keras.layers.Input(shape=(1024,), name=\"input_2\")\n",
    "    inputs_3 = tf.keras.layers.Input(shape=(1024,), name=\"input_3\")\n",
    "        \n",
    "    # Extract features using the encoder\n",
    "    features_1 = encoder1(inputs_1)\n",
    "    features_2 = encoder2(inputs_2)\n",
    "    features_3 = encoder3(inputs_3)\n",
    "\n",
    "    #Merging features\n",
    "    features_concat = tf.keras.layers.concatenate([features_1, features_2, features_3], axis=1)\n",
    "    \n",
    "    outputs_1 = tf.keras.layers.Dense(projection_units_merge, activation=\"relu\")(features_concat)\n",
    "    model = tf.keras.models.Model(inputs=[inputs_1,inputs_2, inputs_3], outputs=outputs_1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19af06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_projection_head_emotion_sentiment(encoder1):\n",
    "    # Inputs for each modality\n",
    "    inputs_1 = tf.keras.layers.Input(shape=(10,), name=\"input_1\")\n",
    "    inputs_2 = tf.keras.layers.Input(shape=(3,), name=\"input_2\")\n",
    "        \n",
    "    # Extract features using the encoder\n",
    "    features_1 = encoder1([inputs_1, inputs_2])\n",
    "\n",
    "    \n",
    "    outputs_1 = tf.keras.layers.Dense(projection_units, activation=\"relu\")(features_1)\n",
    "    model = tf.keras.models.Model(inputs=[inputs_1,inputs_2], outputs=outputs_1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fda70651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_projection_head_caption(encoder1):\n",
    "    # Inputs for each modality\n",
    "    inputs_1 = tf.keras.layers.Input(shape=(1024,), name=\"input_1\")\n",
    "        \n",
    "    # Extract features using the encoder\n",
    "    features_1 = encoder1(inputs_1)\n",
    "\n",
    "    outputs_1 = tf.keras.layers.Dense(projection_units, activation=\"relu\")(features_1)\n",
    "    model = tf.keras.models.Model(inputs=inputs_1, outputs=outputs_1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55351b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
